{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Metrics\n",
    "\n",
    "We will cover these concepts in more depth, but there are some basics to understand around how to evaluate a (binary for now) classifier.\n",
    "\n",
    "### Measuring success\n",
    "\n",
    "So how do we measure how well our model does? Just like with regression, we need to split the data in a training set and a test set and measure our success based on how well it does on the test set.\n",
    "\n",
    "#### Accuracy\n",
    "The simplest measure is **accuracy**. This is the number of correct predictions over the total number of predictions. It's the percent you predicted correctly. In `sklearn`, this is what the `score` method calculates.\n",
    "\n",
    "#### Shortcomings of Accuracy\n",
    "Accuracy is often a good first glance measure, but it has many shortcomings. If the classes are unbalanced, accuracy will not measure how well you did at predicting. Say you are trying to predict whether or not an email is spam. Only 2% of emails are in fact spam emails. You could get 98% accuracy by always predicting not spam. This is a great accuracy but a horrible model!\n",
    "\n",
    "#### Confusion Matrix\n",
    "We can get a better picture our model by looking at what is known as the [**confusion matrix**](https://en.wikipedia.org/wiki/Confusion_matrix). For a binary classification, we get the following four metrics:\n",
    "\n",
    "* **True Positives (TP)**: Correct positive predictions\n",
    "* **False Positives (FP)**: Incorrect positive predictions (false alarm)\n",
    "* **True Negatives (TN)**: Correct negative predictions\n",
    "* **False Negatives (FN)**: Incorrect negative predictions (a miss)\n",
    "\n",
    "|            | Predicted Yes  | Predicted No   |\n",
    "| ---------- | -------------- | -------------- |\n",
    "| Actual Yes | True positive  | False negative |\n",
    "| Actual No  | False positive | True negative  |\n",
    "\n",
    "As you can see these directly to Type I and II error (how)?\n",
    "\n",
    "\n",
    "#### Precision, Recall and F1\n",
    "Instead of accuracy, there are some other scores we can calculate:\n",
    "\n",
    "* **Precision**: A measure of how good your positive predictions are\n",
    "```\n",
    "Precison = TP / (TP + FP)\n",
    "         = TP / (predicted yes)\n",
    "```\n",
    "   \n",
    "* **Recall**: A measure of how well you predict positive cases. Aka *sensitivity*.\n",
    "```\n",
    "Recall = TP / (TP + FN) \n",
    "       = TP / (actual yes)\n",
    "```\n",
    "\n",
    "* **F1 Score**: The harmonic mean of Precision and Recall\n",
    "```\n",
    "F1 = 2 / (1/Precision + 1/Recall)\n",
    "   = 2 * Precision * Recall / (Precision + Recall)\n",
    "   = 2TP / (2TP + FN + FP)\n",
    "```\n",
    "\n",
    "Accuracy can also be written in this notation:\n",
    "```\n",
    "Accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from conf_matrix import plot_confusion_matrix\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "columns = [name[:-5].replace(' ','_') for name in iris.feature_names]\n",
    "iris_df = pd.DataFrame(iris.data, columns=columns)\n",
    "iris_df['target'] = iris.target\n",
    "\n",
    "X = iris_df[iris_df.target > 0]\n",
    "X.drop('target', axis=1, inplace=True)\n",
    "y = iris_df.target[iris_df.target > 0]\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "y_pred = classifier.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "class_names = iris.target_names[1:]\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, \n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision = TP / (TP + FP)\n",
    "precision = ?\n",
    "print(precision)\n",
    "#print(precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall = TP / (TP + FN)\n",
    "recall = ?\n",
    "print(recall)\n",
    "#print(recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "accuracy = ?\n",
    "print(accuracy)\n",
    "#print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifier.coef_)\n",
    "print(classifier.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, coef in zip(columns, classifier.coef_[0]):\n",
    "     print(\"{}: {}\".format(col, coef))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
