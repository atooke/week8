{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "Before you begin fitting a model on a new dataset you should, almost always, split your initial dataset into a \"train\" dataset and a \"test\" dataset. You will use the train dataset to build your model and the test dataset to measure the model's success.\n",
    "\n",
    "You should generally keep 10-50% of the data for the test set and use the rest for training. \n",
    "\n",
    "You should always split your data as randomly as possible. It should be obvious to you that the slightest inclusion of a nonrandom process in the selection of the training set can skew model parameters. Data is frequently sorted in some way (by date or even by the value you are trying to predict). *Never* just split your data into the first 90% and the remaining 10%. *Why would this be bad?*\n",
    "\n",
    "Luckily for us, there is a nice method implemented in scipy that splits the dataset randomly for us called [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html). If you do not happen to have this tool in the language you're working in (i.e. Java), you'll need to make it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin in-class [exercise-0](./in-class-exercise-0.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFold Cross Validation\n",
    "\n",
    "One thing about doing standard cross validation is that, as observed above, your score can depend on the nature of the random split. We can get a more accurate value of the error by using *KFold Cross Validation*. \n",
    "\n",
    "The algorithm proceeds as follows:\n",
    "\n",
    "1. Break the data randomly into k groups.\n",
    "1. Take one of these groups and make it the test set. \n",
    "1. Train the model on the remaining groups and calculate the error on the test group. \n",
    "1. This process k times and average all the results. This gives us a more accurate picture of the error.\n",
    "\n",
    "The advantage of this method over repeated random sub-sampling (see below) is that all observations are used for both training and validation, and each observation is used for validation exactly once. 10-fold cross-validation is typical, but $k$ can be any value (rarely more than 20).\n",
    "\n",
    "For example, setting k = 2 results in 2-fold cross-validation. In 2-fold cross-validation, we randomly shuffle the dataset into two sets d0 and d1, so that both sets are equal size (this is usually implemented by shuffling the data array and then splitting it in two). We then train on d0 and validate on d1, followed by training on d1 and validating on d0.\n",
    "\n",
    "When k = n (the number of observations), the k-fold cross-validation is exactly the leave-one-out cross-validation.\n",
    "\n",
    "In **stratified k-fold cross-validation**, the folds are selected so that the mean response value ($y$) is approximately equal in all the folds. This means that we need to undertake the process of stratification of the response variable if it is not directly separable into distinct classes. If it is, the stratification requires that the proportions of the classes in each split be approximately the same. \n",
    "\n",
    "\n",
    "[Link to more information](http://cse3521.artifice.cc/classification-evaluation.html)\n",
    "\n",
    "\n",
    "There are many other ways to do cross validation. \n",
    "\n",
    "Additional reading:\n",
    "1. http://people.csail.mit.edu/romer/papers/CrossVal_SDM08.pdf\n",
    "2. http://web.cs.iastate.edu/~jtian/cs573/Papers/Kohavi-IJCAI-95.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin in-class [exercise-1](./in-class-exercise-1.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
